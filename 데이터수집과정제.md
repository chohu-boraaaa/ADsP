# 1. 빅데이터 수집과 정제
## 1 | 빅데이터의 수집

### 1. 정형 반정형 비정형 데이터

* 정형 데이터 : 관계형 데이터베이스의 고정된 필드에 저장, 데이터베이스 스키마 지원 ex. RDB, 스프레드시트

* 반정형 데이터 : 데이터의 속성인 메타데이터를 가지고 있으며 스토리지(Storage)에 저장되는 데이터 파일, 데이터의 값과 형식 일관성 X ex. HTML, XML, JSON, 웹페이지, 웹로그, 센서데이터

* 비정형 데이터 : 텍스트 데이터, 멀티미디어 데이터 ex. 소설데이터, 문서, 이미지, 오디오, 비디오

### 2. 데이터 유형별 수집기술

* 확장성, 실시간성, 유연성 확보해야 함

* 수집기술 선정 시, 정제 및 변환과정, 전처리 및 프로세스 유무 확인

* 정형 데이터 : ETL, FTP, Open API

* 반정형 데이터 : Crawling, RSS, Open API, FTP

* 비정형 데이터 : Crawling, RSS, Open API, FTP, Streaming

### 3. 데이터 수집기술

* Crawling : 웹페이지 정보 수집

* FTP : 인터넷 프로토콜 이용하여 파일 수집

* Open API : SNS 정보를 API를 이용해서 실시간으로 데이터 수집

* RSS : XML기반으로 블로그, 뉴스 등의 컨텐츠 수집

* Streaming : 멀티미디어 데이터를 실시간으로 수집

* Log Aggregator(로그 수집기) : 웹로그, 웹서버로그 등을 수집

* RDB(RDB 수집기) : 관계형 데이터베이스에서 정형화된 데이터를 수집하여 분산파일시스템이나 HBase와 같은 NoSQL에 저장

### 4. 데이터 유형

#### 4-1. 내부 데이터 베이스 데이터

- 정형화 되어 있음
- 참조 무결성 제약조건으로 인해 일부 오류 데이터 존재 가능
- 데이터 품질 검사 필요 : 데이터 프로파일링
- SQL을 사용해서 데이터베이스 내의 테이블 확인 가능

#### 4-2. 텍스트 데이터
- 전처리 과정을 통해 정형화한 후 빈도분석을 하는 텍스트 마이닝에 사용됨
- 일반 문서 : 워드, 엑셀, 한글, 메모장 같은 파일에 저장된 텍스트 데이터
- 웹페이지 : 웹은 페이지 단위로 구성, 페이지 내에 텍스트 있음

#### 4-3. SNS 데이터
- SNS에 있는 데이터를 API로 수집 가능
- JSON 형태로 데이터 제공
- ex. Facebook : 페이스 그래프 API를 사용해서 수집, HTTP 프로토콜 사용
- 페이스북 그래프 API 읽기 : GET/[version info]/[Node|Edge Name]
- 페이스북 데이터 JSON 형태로 응답 : {"fieldname"(필드명) : {field-value}(필드값), ... }

#### 4-4. 로그(Log) 데이터
- 정보시스템에서 자동으로 생성되는 로그파일 수집 가능
- 로그파일은 계속해서 발생하는 데이터로 시스템별로 로그파일 저장하는 방식 다름
- 웹서버 access.log 파일 : 사용자가 웹페이지에 접속 -> 사용자의 웹서버 호출과 응답로그 발생 -> IP주소, 일자와 시간, 호출 웹페이지, 브라우저 종류 등을 확인 가능

#### 4-5. 센서 데이터
- IoT는 수 많은 사물에 센서를 설치하고 센서에서 전송하는 정보를 수신하여 사용자에게 서비스 제공
- IoT의 센서에서 전송하는 사물정보를 수집하여 데이터 분석

#### 4-6. 음성 데이터
- 사용자의 음성 데이터는 아날로그 데이터
- 디지털로 변환해야 함 이를 PCM과정이라고 함
- 디지털을 텍스트로 신경망 알고리즘을 이용하여 변환
- STT(Speech to text), TTS(text to Speech)를 지원

#### 4-7. 이미지 데이터
- 픽셀 추출하여 데이터 분석
- Open CV : 이미지 데이터에서 픽셀 가져옴, 이미지를 활용해서 머신러닝 할 수 있도록 함

### 5. 데이터 속성 파악
경험적 측면으로 구분

#### 5-1. 휴먼 빅데이터
- 사람의 행위에 대한 정보를 저장하고 있는 데이터
- ERP, SCM, MES, CRM, DW(Data Warehouse), VOC 등에 저장되어 있음

#### 5-2. 시스템 빅데이터
- 서버, 데이터베이스 로그, 보안장비 등과 같이 시스템 내부에서 저장하는 데이터

#### 5-3. 소셜 빅데이터
- 웹뉴스, SNS 등에서 수집되는 빅데이터

#### 5-4. 행태 및 상태 빅데이터
- 사람의 행위에서 발생하는 데이터 ex. 통화기록 데이터

### 6. 데이터 특성 파악

#### 6-1. JSON
- 속성과 값의 표현식으로 사람과 기계 모두 이해하기 쉬움
- XML 대체해서 데이터 전송에 많이 쓰임

#### 6-2. Open API
- 누구나 인터넷에 있는 정보를 사용할 수 있도록 공개된 API
- 표준화된 인터페이스를 사용해서 콘턴체를 읽어올 수 있음
- 실시간 데이터 수집을 위해 활용

#### 6-3. RSS
- 뉴스 혹은 블로그에서 사용되는 콘텐츠 표현 방식
- 새로운 글 게시되면 자동 통보

#### 6-4. WSNs
- 넓은 공간 관측을 위해 수많은 센서를 배포하여 네트워크 형성

#### 6-5. 크롤링
- 웹사이트를 방문해서 원하는 정보를 스스로 추출하는 것
- 크롤러 : 웹사이트를 탐색해서 정보 추출하는 프로그램

## 2 | 적합한 품질의 데이터로 변환

### 1. 데이터 변환 방법

#### 1-1. 표준화 & 정규화
- 표준화 : 값의 스케일이 다른 두 개의 변수가 있을 때 이 변수들의 스케일 차이를 제거
- 정규화 : 데이터를 0과 1로 변환하여 데이터 분포를 조정

##### 1) Z 변환
- 중심극한정리에 근거하여 표본 추출하는 경우 표본은 정규분포에 근사한다고 가정
- (해당값 - 평균) / 표준편차

##### 2) [0-1]변환
- 연속형 변수를 0~1 사이 값으로 변환
- 변수들 간의 스케일이 다를 때 0~1 사이 값으로 표준화한 후에 분석
- (해당 값 - 최소값) / (최대값 - 최소값)

#### 1-2. 정규분포화
로그 변환과 제곱근 변환 중 정규분포를 가장 잘 나태는 것으로 사용

##### 1) 로그(log) 변환
- 입력되는 데이터가 멱함수 분포를 나타내는 경우 log를 사용해서 정규분포로 변환
- 멱함수 분포 : 한 수가 다른 수의 거듭제곱으로 표현되는 두 수의 함수적 관계 ex. 20%의 결함이 80%에 영향을 준다는 경험적 법칙

##### 2) 제곱근 변환
- 정규분포가 아닌 데이터를 제곱근을 사용해서 정규분포 변환

cf) 변환함수 별 분포형태
- X^3 : left
- X^2 : mild left
- sqrt(X) : mild right
- ln(X) : right
- 1/X : severe right

#### 1-3. 범주화

##### 1) 이산형화
- 범주형 변수를 가변수로 변환하여 분석 수행
- 이산화 : 다수의 구간으로 연속형 변수를 범주화

##### 2) 이항변수화
- 0과 1의 두 개의 값으로 가변환

#### 1-4. 개수축소
데이터에서 확률표본 추출 기법을 사용해서 개수축소를 할 수 있다

##### 확률표본 추출 기법
- 단순 임의 추출 : 모집단에서 균등하게 임의로 추출
- 체계적 추출 : 모집단에서 시간적, 공간적 간격을 두고 표본 추출
- 층화 임의 추출 : 모집단의 몇 개로 분류하고 분류 내에서 임의 추출
- 군집 추출 : 모집단을 특정 군집으로 분류하고 군집 내의 모든 데이터를 표본으로 추출
- 다단계 추출 : 표본을 추출할 때 단계를 나누어 표본을 추출 ex. 1단계에서는 군집추출하고 2단계에서는 층화 임의추출

#### 1-5. 차원축소

##### 1) 요인분석(FA)
- 수많은 변수 중에서 잠재적인 몇 개의 변수를 찾는 방법으로 공통적인 요인을 찾아냄
- 잠재적인 변수를 찾기 때문에 새로운 변수가 생성되는 효과
- ex. 성적을 구성하는 변수 중 수학점수, 과학점수에 잠재된 요인은 수리적 능력이고 영어점수, 중국어점수에 잠재된 요인은 어학능력이다.
- 장점 : 입력 변수들 간의 상관관계를 파악하고 데이터 특성을 이해할 수 있고, 분석모델에 잠재적인 변수를 추출하여 새로운 변수를 생성하는 효과 있고, 변수의 수를 줄일 수 있어 분석결과에 대한 해석 용이, 독립변수 간의 상관관계가 있는 다중공선성의 문제는 잠재적 변수를 추가하여 해결 가능

##### 2) 주성분분석(PCA)
- 변수를 선택하거나 소수의 주성분 혹은 요인으로 축소하는 방법
- 상관성이 높은 변수로 차원을 축소하여 모형을 개발하면 데이터에 대한 이해가 쉽고 관리가 편리
- 종속변수 Y에 가장 영향을 주는 변수를 선택해서 차원을 축소

cf) 데이터 변환 방법
평활화 : 데이터로부터의 잡음 제거를 위해 추세에 벗어나는 값들을 변환
집계 : 다양한 차원의 방법으로 데이터 요약
일반화 : 특정 구간에 분포하는 값으로 스케일 변환
정규화 : 최소, 최대 정규화 및 Z-스코어 정규화, 소수 스케일링 방법 등 통계적 기법 사용
속성생성 : 데이터 통합을 위해 새로운 속성이나 특징 만듦, 여러 데이터 분포를 대표할 수 있는 새로운 속성과 특징을 활용

#### 1-6. 시그널 데이터 변환

##### 1) 푸리에 변환
- 시간에 따른 진폭 데이터를 주파수별 세기로 변환

##### 2) 웨이블릿 변환
- 푸리에 변환은 모든 주파수가 혼합되어 있는 형태로 나오는 문제가 있어 이를 해결한 것이 웨이블릿 변환
- 작은파(웨이블릿)을 패턴으로 하여 천이하거나 확대, 축소의 스케일을 사용해서 임의의 파형으로 표현

### 2. 데이터 변환 후 품질 검증
데이터 품질 검증은 빅데이터 수집, 정제, 저장, 분석 등 모든 과정에서 지속적으로 이루어져야 함

#### 2-1. 빅데이터 수집단계 품질 확인사항
- 수집기준의 타당성(근거, 통계적 유의성)이 확보되어 있는지 확인
- 데이터 추출 조건에 맞는 정보의 관련 항목 모두가 추출되었는지 확인
- 악의적 유포 데이터 제거방법을 확보하였는지 확인

#### 2-2. 빅데이터 저장관리
- 빅데이터에 저장할 때 누락된 데이터 없는지 확인
- 빅데이터 저장을 위한 키 구성의 적절성 평가
- 파일 검증 필요
- 저장 레이아웃에 따른 검증 필요

#### 2-3. 빅데이터 분석 활용
- 최신성 확인
- 충분한 정보 확인
- 원하는 정보인지 확인

#### 2-4. 빅데이터 품질관리 확인 방법
- 내부 사용자 설문조사
- 피드백
- 만족도 조사
- 사후 고객 만족도 분석

## 3 | 빅데이터의 정제

### 1. 데이터 정제 절차
정제하지 않으면 데이터 일관성, 무결성 등에 문제 발생
데이터 표준에 맞게 변환하고 비정형 데이터를 정형데이터로 변환
유사 데이터를 통합하는 과정 수행

#### 1-1. 빅데이터 정제 절차

##### 1) 빅데이터 전처리
- 구조화되지 않는 비정형 데이터를 분석 가능한 구조화된 데이터로 교정하는 과정
- 전처리 과정 : 데이터 변환 -> 데이터 교정(결측치 변환, 이상치 제거, 노이즈 데이터 교정, 비정형 데이터는 반드시 수행) -> 데이터 통합
- 전처리 기술 : 데이터 여과(오류 발견, 보정, 삭제, 중복확인을 통해 데이터 품질 향상), 데이터 변환(정규화, 집합화, 요약화, 계층생성 방법 사용, ETL도구 제공), 데이터정제(결측치에 데이터 삽입 및 이상치 식별 혹은 제거, 잡음의 평활화를 통해 불일치성 교정), 데이터 통합, 데이터 축소(활용되지 않는 항목 제거)

##### 2) 빅데이터 후처리
- 확보된 데이터를 저장하고 지속적인 품질관리 수행
- 빅데이터 저장소 용량 산정하고 저장
- 데이터 저장 후에는 집계, 일반화, 정규화를 수행하여 일관성 향상
- 빅데이터 저장 방식 : RDB(관계형 데이터베이스에 저장하므로 SQL을 사용해 쉽게 편리하게 관리 가능, Oracle, MySQL), NoSQL(Not Only SQL, Mongo DB, HBase), 분산 파일 시스템(분산 서버를 사용해서 여러 서버에 분산하여 저장, 대규모 저장소를 제공하고 성능 향상, HDFS(Hadoop Distributed File System))

### 2. 데이터 정제 기술
불일치성을 교정하기 위해서 결측치 처리, 잡읍 처리 기술을 활용

#### 2-1. 데이터 정제 방법 - 결측치
- 해당 레코드 무시 : 각 클래스의 라벨 구분이 없는 경우 해당 레코드를 무시, 결측치가 많을 경우 매우 비효율적
- 자동으로 채우기 : 결측치 값을 'unknown'으로 입력, 전체 평균값/중앙값/해당 클래스와 같은 레코드에 속한 데이터의 평균값으로 입력, 베이지안 확률추론과 결정트리를 이용하여 추정치 적용
- 담당자(전문가) 수작업 입력 : 담당자가 데이터를 확인하고 적절한 값으로 수정, 작업에 많은 시간이 필요하지만 높은 신뢰성 얻을 수 있음

#### 2-2. 데이터 정제 방법 - 잡음처리
- 잡음 : 랜덤에러나 측정된 변수의 변형된 값
- 잡음이 발생하는 이유 : 센서 작동실패, 데이터 엔트리 표기 문제, 데이터 전송 오류, 기술적 한계, 데이터 속성 값의 부정확성
- 구간화 : 여러 개의 구간으로 나누고 구간에 있는 값들을 대푯값으로 대체
- 회귀값 적용 : 데이터를 잘 표현하는 추세함수를 찾아 이 함수 값으로 적용
- 군집화 : 비슷한 성격을 가지고 있는 클러스터 단위로 군집한 후 극단치 제거
